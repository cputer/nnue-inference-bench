// mind-cpu/nnue_infer.mind - SIMD-optimized NNUE inference
// Target: Beat C++ baseline (~81k pos/s)

import std.mem
import std.simd
import std.math

const nnue_model = @import("nnue_model.mind");
const NKNNModel = nnue_model.NKNNModel;
const L1_OUTPUT = nnue_model.L1_OUTPUT;
const L2_INPUT = nnue_model.L2_INPUT;
const L2_OUTPUT = nnue_model.L2_OUTPUT;
const L3_OUTPUT = nnue_model.L3_OUTPUT;

pub const Position = struct {
    pieces: [64]i8,     // -1=empty, 0-4=WP..WQ, 5=WK, 6-10=BP..BQ, 11=BK
    white_king: u8,
    black_king: u8,
    stm: u8,            // 0=white, 1=black
};

// SCReLU: squared clipped ReLU
inline fn screlu(x: f32) f32 {
    const clipped = math.clamp(x, 0.0, 1.0);
    return clipped * clipped;
}

// SIMD-optimized SCReLU for vectors
inline fn screlu_vec(comptime N: usize, v: @Vector(N, f32)) @Vector(N, f32) {
    const zero = @splat(N, @as(f32, 0.0));
    const one = @splat(N, @as(f32, 1.0));
    const clipped = @max(zero, @min(one, v));
    return clipped * clipped;
}

pub fn extract_halfkp_features(pos: *const Position, features_w: *[64]i32, features_b: *[64]i32) struct { w_count: usize, b_count: usize } {
    var w_count: usize = 0;
    var b_count: usize = 0;

    for pos.pieces |piece, sq| {
        if piece < 0 or piece == 5 or piece == 11 {
            continue;  // Empty or king
        }

        const halfkp_piece: i32 = if piece < 5 then piece else piece - 1;

        // White perspective
        const w_feat = @intCast(i32, pos.white_king) * 640 + halfkp_piece * 64 + @intCast(i32, sq);
        features_w[w_count] = w_feat;
        w_count += 1;

        // Black perspective (flipped)
        const flipped_sq = sq ^ 56;
        const flipped_king = pos.black_king ^ 56;
        const b_halfkp_piece: i32 = if halfkp_piece < 5 then halfkp_piece + 5 else halfkp_piece - 5;
        const b_feat = @intCast(i32, flipped_king) * 640 + b_halfkp_piece * 64 + @intCast(i32, flipped_sq);
        features_b[b_count] = b_feat;
        b_count += 1;
    }

    return .{ .w_count = w_count, .b_count = b_count };
}

// SIMD-optimized sparse accumulation
fn sparse_accum_simd(model: *const NKNNModel, features: []const i32, count: usize, out: *[L1_OUTPUT]f32) void {
    // Start with bias
    const VEC_SIZE = 8;  // AVX2 float vector
    comptime var i: usize = 0;
    inline while (i < L1_OUTPUT) : (i += VEC_SIZE) {
        const bias_vec: @Vector(VEC_SIZE, f32) = model.B1[i..][0..VEC_SIZE].*;
        out[i..][0..VEC_SIZE].* = bias_vec;
    }

    // Add feature weights (this is the hot loop)
    for features[0..count] |feat_idx| {
        if feat_idx < 0 or feat_idx >= 40960 {
            continue;
        }
        const row_start = @intCast(usize, feat_idx) * L1_OUTPUT;

        // SIMD accumulation
        comptime var j: usize = 0;
        inline while (j < L1_OUTPUT) : (j += VEC_SIZE) {
            const w_vec: @Vector(VEC_SIZE, f32) = model.W1[row_start + j..][0..VEC_SIZE].*;
            const acc_vec: @Vector(VEC_SIZE, f32) = out[j..][0..VEC_SIZE].*;
            out[j..][0..VEC_SIZE].* = acc_vec + w_vec;
        }
    }
}

// Dense layer with SIMD
fn dense_layer_simd(comptime IN: usize, comptime OUT: usize, weights: []const f32, bias: []const f32, input: []const f32, output: *[OUT]f32, apply_screlu: bool) void {
    const VEC_SIZE = 8;

    for output |*o, j| {
        var sum: f32 = bias[j];

        // SIMD dot product
        comptime var i: usize = 0;
        var acc: @Vector(VEC_SIZE, f32) = @splat(VEC_SIZE, @as(f32, 0.0));

        inline while (i + VEC_SIZE <= IN) : (i += VEC_SIZE) {
            const w_vec: @Vector(VEC_SIZE, f32) = weights[j * IN + i..][0..VEC_SIZE].*;
            const in_vec: @Vector(VEC_SIZE, f32) = input[i..][0..VEC_SIZE].*;
            acc += w_vec * in_vec;
        }

        // Horizontal sum
        sum += @reduce(.Add, acc);

        // Handle remainder
        while (i < IN) : (i += 1) {
            sum += weights[j * IN + i] * input[i];
        }

        o.* = if apply_screlu then screlu(sum) else sum;
    }
}

pub fn forward(model: *const NKNNModel, features_w: []const i32, w_count: usize, features_b: []const i32, b_count: usize, stm: u8) f32 {
    var acc_white: [L1_OUTPUT]f32 = undefined;
    var acc_black: [L1_OUTPUT]f32 = undefined;

    // L1: Sparse accumulation (SIMD optimized)
    sparse_accum_simd(model, features_w, w_count, &acc_white);
    sparse_accum_simd(model, features_b, b_count, &acc_black);

    // Concatenate with SCReLU based on STM
    var hidden1: [L2_INPUT]f32 = undefined;
    if stm == 0 {  // White to move
        for acc_white |v, i| {
            hidden1[i] = screlu(v);
        }
        for acc_black |v, i| {
            hidden1[L1_OUTPUT + i] = screlu(v);
        }
    } else {  // Black to move
        for acc_black |v, i| {
            hidden1[i] = screlu(v);
        }
        for acc_white |v, i| {
            hidden1[L1_OUTPUT + i] = screlu(v);
        }
    }

    // L2: Dense 512->32 with SCReLU
    var hidden2: [L2_OUTPUT]f32 = undefined;
    dense_layer_simd(L2_INPUT, L2_OUTPUT, &model.W2, &model.B2, &hidden1, &hidden2, true);

    // L3: Dense 32->32 with SCReLU
    var hidden3: [L3_OUTPUT]f32 = undefined;
    dense_layer_simd(L2_OUTPUT, L3_OUTPUT, &model.W3, &model.B3, &hidden2, &hidden3, true);

    // L4: Dense 32->1 (no activation)
    var eval_score: f32 = model.B4;
    for hidden3 |h, i| {
        eval_score += h * model.W4[i];
    }

    return eval_score;
}
